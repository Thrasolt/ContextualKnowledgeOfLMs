{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a0caa5-768b-48f5-bd0f-235c3e12b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize, log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75241d3a-61e8-4f35-b3b7-8a5dd0dadb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TRExData.LamaTRExData import LamaTRExData\n",
    "from ResultData.ScorePersistor import ScorePersistor\n",
    "from relation_templates.templates import get_templates, get_relation_meta, relations, relation_names, get_relations_with_last_digit, nominalized_relations\n",
    "from ModelHelpers.fill_mask_helpers import get_probability_from_pipeline_for_token\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be087098-0bcc-40f6-8d93-ea7de128e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f72848f5-e838-4ea3-9018-9af6cdaad93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE = 'simple'\n",
    "COMPOUND = 'compound'\n",
    "COMPLEX = 'complex'\n",
    "COMCOM = \"compound-complex\"\n",
    "\n",
    "#KEYS = [SIMPLE, COMPOUND, COMPLEX, COMCOM]\n",
    "KEYS = [\"active\", \"passive\", \"nominalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f31c2d-fef5-4e07-94cb-6a2d7f7f1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model='bert-base-cased'\n",
    "model='bert-base-uncased'\n",
    "#model='bert-large-cased'\n",
    "#model='bert-large-uncased'\n",
    "#model='bert-base-multilingual-cased'\n",
    "#model='bert-base-multilingual-uncased'\n",
    "VOCABULARY_SIZE = 28996\n",
    "MASK = \"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9d3481-3809-495e-9f85-a492aba1e795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, ['P127', 'P136', 'P176', 'P178', 'P413', 'P1303'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations = nominalized_relations[2:]\n",
    "len(relations), relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "390da65a-543e-4703-a4dd-b6c3c1d9b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask', model=\"bert-base-cased\", top_k=VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b7efaf-9113-4ef2-b351-8facc6bc9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "TREx = LamaTRExData(relations = relations)\n",
    "TREx.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75e6e30f-a2fd-4cf6-b7a9-aec625ad5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "persister = ScorePersistor(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc24dd86-85a8-4bfa-9417-d326e10da456",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def metric(sentence: str, token: str):\n",
    "    prob = get_probability_from_pipeline_for_token(unmasker(sentence), token)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e4fd20a-b510-4a20-a1d3-ced8ce7240df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 687/687 [1:09:39<00:00,  6.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 931/931 [33:21<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                                                          | 2/982 [00:04<39:30,  2.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m get_templates(relation, sub, MASK, KEYS)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m KEYS:\n\u001b[0;32m----> 6\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         persister\u001b[38;5;241m.\u001b[39mpersist_score_row(key, relation, sub, obj , score)\n",
      "File \u001b[0;32m~/projects/ContextualKnowledgeOfLMs/newMA/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [10], line 3\u001b[0m, in \u001b[0;36mmetric\u001b[0;34m(sentence, token)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetric\u001b[39m(sentence: \u001b[38;5;28mstr\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     prob \u001b[38;5;241m=\u001b[39m get_probability_from_pipeline_for_token(\u001b[43munmasker\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m, token)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prob\n",
      "File \u001b[0;32m~/projects/ContextualKnowledgeOfLMs/newMA/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py:227\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m        - **token** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/ContextualKnowledgeOfLMs/newMA/lib/python3.9/site-packages/transformers/pipelines/base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/ContextualKnowledgeOfLMs/newMA/lib/python3.9/site-packages/transformers/pipelines/base.py:1082\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1080\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m   1081\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m-> 1082\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/projects/ContextualKnowledgeOfLMs/newMA/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py:140\u001b[0m, in \u001b[0;36mFillMaskPipeline.postprocess\u001b[0;34m(self, model_outputs, top_k, target_ids)\u001b[0m\n\u001b[1;32m    136\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens[np\u001b[38;5;241m.\u001b[39mwhere(tokens \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Originally we skip special tokens to give readable output.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# For multi masks though, the other [MASK] would be removed otherwise\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# making the output look odd, so we add them back\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m proposition \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m: p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_str\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(p), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence}\n\u001b[1;32m    142\u001b[0m row\u001b[38;5;241m.\u001b[39mappend(proposition)\n",
      "File \u001b[0;32m~/projects/ContextualKnowledgeOfLMs/newMA/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3432\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3430\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/ContextualKnowledgeOfLMs/newMA/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:549\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    548\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 549\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n\u001b[1;32m    552\u001b[0m     clean_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization(text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for relation in relations:\n",
    "    print(relation)\n",
    "    for sub, obj in tqdm(TREx.data[relation]):\n",
    "        sentences = get_templates(relation, sub, MASK, KEYS)\n",
    "        for key in KEYS:\n",
    "            score = metric(sentences[key], obj)\n",
    "            try:\n",
    "                persister.persist_score_row(key, relation, sub, obj , score)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(relation, sub, obj)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae6092b-8b45-468b-b111-e362e53abeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "persister.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b36e95-a752-42eb-9c97-8b02d12ea7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
